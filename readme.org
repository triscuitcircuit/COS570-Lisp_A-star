#+Author: Tristan Zippert
#+STARTUP: showeverything 

#+LATEX_CLASS_OPTIONS: [11pt]
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{enumitem}

#+LaTeX_HEADER: \setlist{leftmargin=0.25in,nosep}
#+LaTeX_HEADER: \documentclass[10pt,a4paper,showtrims]{memoir}
#+LaTex_HEADER: \usepackage[labelfont=bf]{caption}
#+LaTeX_HEADER: \hypersetup{colorlinks=true, urlcolor={blue}, linkcolor={blue}}

#+LATEX_HEADER: \usepackage[natbib=true]{biblatex}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{shapes.misc,shadows,arrows}


#+LaTeX_HEADER: \usepackage{sectsty}
#+LATEX_HEADER: \usepackage{parskip}


#+OPTIONS: h:3 toc:nil
#+OPTIONS:
#+STARTUP: inlineimages
#+TITLE: Agents and Search Report
#+SUBTITLE: COS 570:  University of Maine School of Computing and Information Science
\tableofcontents
\clearpage
* Introduction
  For this report, the topic of symbolic search agents is explored in
  simulated "Robot World" domain.  Within the simulations 
  there exists a defined goal for the agents as well as areas that are
  blocked from agent movement. The agents within the Robot World domain have the ability to sense their
  surroundings with the use of sensor perception.
  Specifically this project outlines the usage of a Reflex Agent,
  a Model-Based reflex agent, a Hill-Climb  agent, a Uniform Cost Search agent
  , and a A* Search Agent. 
  This report also showcases those same search algorithms
  when applied to another domain,
  comparing the results between the two mentioned domains. 
* Methods
  This project makes use of the programming language Common Lisp as a means of programming the search and reflex agents.
  Use of Common Lisp's macro system was the basis of measurements, specifically a macro  that timed the
  total time of function execution.
  #+NAME: cost-timer
  #+CAPTION: "cost-timer" Macro that measured the execution time of input functions
  #+begin_src lisp -n 
	(defmacro cost-timer( ti  &body body)
	  `(cond(ti
		 (time(progn ,@body))) 
	 (t(progn ,@body))))
  #+end_src
  A class instance variable for the search agents was also used to keep track of the number of nodes created for their
  internal data structure. The time it takes for search agents to find a goal when searching and the amount of
  nodes created are compared using a statistical paired t-test.
  
  
** Agents
   During the implementation of agents, drastic changes had to be made for each type of reflex agent. 
   However, due to the inherit similarities, there were only slight changes between the A* agent
   and the Uniform Cost Search Agent. 
*** Reflex Agent
    The Reflex Agent for the domain of Robot World was implemented in a way
    where it constantly moved forward until it reached a corner. If it encountered an obstacle when it was moving
    in the ~forward~ direction, it would move in the ~right~ direction. Due to this being a reflex agent, it had
    no memory of its past sensor activations and only relied on the current sensor input received. When the
    Reflex Agent encountered the bounds of the simulation
    (or a corner) it will continue to try and move forward, only to be prevented from moving by the simulation.
*** Model based Reflex Agent
    The Model Based Reflex Agent is similar to the Reflex agent, however, this agent type has past percept
    values that it can refer to when choosing a next action. The goal for this agent is to find a way to the closest
    corner of the bounds of the simulator, similar to the Reflex agent. 
*** Hill Climb Agent
    The Hill Climb Agent type is a different type of agent compared to the Reflex Agents in the sense that
    it has a defined goal location in the simulator. It finds its way to the goal with a heuristic function to
    determine the best action it can take to get it closer to the goal. The movement commands are stored in
    a list along with the heuristic value and each result is stored in a array. The agent then iterates
    over the array and chooses the action with the lowest heuristic value.
    #+CAPTION: Heuristic function for the Hillclimb agent.
    #+begin_src lisp -n      
      (defun heuristic (current goal percept &rest perceptv)
	"takes the current location of the robot and the percept
	values and builds an array of values and direction"
	(setf rtr (make-array '(4)))
	(let ((iteration 0))
	  (loop for n in perceptv do
	    (let ((dX 0)(dY 0)(cmb 0))
	      (cond
		((equal n :forward)(setq current (list (car current) (+ (cadr current ) 1))))
		((equal n :backward)(setq current (list (car current) (- (cadr current ) 1))))
		((equal n :right)(setq current (list (+ (car current) 1) (cadr current ) )))
		((equal n :left)(setq current (list (- (car current)1) (cadr current )))))
	      (setq dX (abs (- (car current) (car goal))))
	      (setq dY (abs (- (cadr current) (cadr goal))))
	      (setq cmb (+ dX dY))
	      (if (sensor-value percept n)
		  (setq cmb 100))  
	      (setf (aref rtr iteration) (list cmb n))
	      (setq iteration (+ iteration 1))
	      )))rtr)
    #+end_src
*** Uniform Cost Agent
    The Uniform Cost Agent uses the same code as the A* Search Agent (as showcased in Figure [[fig:agent-class]] and Figure [[fig:generation]]),
    however it has a modification of using the same ~node-score~ value for each node on the priority queue.
    
    #+begin_src lisp -n
(defmethod node-score((self uniform-agent) node)
  "Assigns score based on g- the parents value- and Manhattan distance heuristic"
  1 )
    #+end_src
*** A*-Search Agent
    Before moving in the world and finding the goal just through heuristic based movement,
    this agent searches all locations (treating them as nodes) until it encounters the goal. Once the
    goal location is found, it traces the parent nodes and builds a list of commands to execute to reach the goal.
    The A* agent is implemented with it having a list of locations that it has visited in its search,
    commands it needs to execute when it found a goal location, and the current ~a-node~ based priority queue.
    #+LABEL: fig:agent-class
    #+CAPTION: A* Star class defintion with instance variables.
    #+begin_src lisp -n
 (defclass a*-search (sim:robot)
  ((blocks :accessor blocks :initform nil :initarg :blocks)
   (visited-list :accessor visited-list :initform '())
   (goal :accessor goal :initform '( 3 4) :initarg :goal)
   (open-list :accessor open-list :initform (make-instance `priority-queue :compare-function #'cmp-nodes))
   (command-list :accessor command-list :initform '())
   (bounds :accessor bounds :initform '(25 25) :initarg :bounds )
   (goal-found :accessor goal-found :initform nil)
   (ti :accessor ti :initform t :initarg :ti)
   (nodes-created :accessor nodes-created :initform 0)
   (robo-loc :accessor robo-loc :initform '(1 1) :initarg :robo-loc)
   (name :accessor name :initform (sym:new-symbol 'a*-search))))
    #+end_src
    The search program was also made use of a node generation function that enqueues
    nodes to the A* search class priority queue. It generates nodes based off a input location,
    associates the last location checked and will use the previous nodes command when building the
    robots command list. \\
    #+LABEL: fig:generation
    #+CAPTION: Node generation function used by the A* agent.  
    #+begin_src lisp -n
(defmethod generation((self a*-search) new-loc command curr-score Parentnode)
  (with-slots (open-list visited-list goal-found nodes-created) self
    (let ((node nil))
      (setq node (make-instance 'a-node :parent Parentnode :coord new-loc :command command))
      (setf (slot-value node 'g) (+ (slot-value Parentnode 'g) 1))
      (setf (slot-value node 'cost) (node-score self node))
      (if (and (not(is-visited self new-loc )) (is-obstacle self new-loc) (in-bounds self new-loc)
	       (not(goal-found self)) 
	       )
	  (progn
	    (incf nodes-created)
	    (enqueue open-list node)
	    (push new-loc visited-list))))))      
    #+end_src 
* Results
  \\
When comparing the results of the search agents to that of the performance of reflex agents, the search agents performed
drastically better in regards to finding the best path to the goal. Due to Hill-climbs inability to backtrack,
the hill-climb agent kept getting stuck against obstacles and couldn't move any closer to the given goal location.

** Result Comparison
*** Search Agents
    The total time of search agents finding a route to a goal
    is recorded in Table [[tbl:1]], while the total amount of nodes generated for each agent is in Table [[tbl:2]].
    \\
    A paired /t-test/ value of 0 was gathered from the time, in seconds, that it took for a search agent to find its
    way to the goal (as shown in Table [[tbl:1]]). Using the paired /t-test/ value of 0, a 
    /p-value/ of 1 was computed showing that results are not left by chance.
    The /p-value/ of the generated node amount value was also determined to be a 0, based off the /t-test/
    value of 1. 
#+LABEL: tbl:1    
#+CAPTION: Time (in seconds) of search agents computing a route to their goal in a simulator of 10 random obstacles
      | A*-Search | Uniform cost |
      |-----------+--------------|
      |  0.000209 |     0.001703 |
      |  0.000393 |      0.00305 |
      |  0.000434 |     0.003692 |
      |  0.000785 |     0.002672 |
      |  0.000233 |     0.003043 |
      |  0.000928 |     0.002914 |
      |  0.000274 |      0.00311 |
      |  0.000504 |     0.002973 |
      |  0.000521 |     0.003982 |
      |  0.000257 |      0.00445 |
      |  0.000463 |     0.003605 |
      |  0.000401 |      0.00281 |
      |  0.000548 |      0.00387 |
      |  0.000232 |     0.003604 |
      |  0.000319 |     0.003656 |
      |  0.000419 |     0.001577 |
      |  0.000513 |     0.003145 |
      |  0.000626 |     0.002749 |
      |  0.000412 |     0.002995 |
#+LABEL: tbl:2
#+CAPTION: Chart showing generated node amount between search agents
| A*-Search | Uniform Cost |
|-----------+--------------|
|        92 | 339          |
|        89 | 336          |
|        88 | 336          |
|       113 | 340          |
|        87 | 335          |
|        88 | 337          |
|        90 | 337          |
|        90 | 335          |
|        88 | 335          |
|        92 | 339          |
|        90 | 338          |
|        90 | 339          |
|        89 | 337          |
|        92 | 335          |
|        92 | 335          |
|        88 | 341          |
|        92 | 336          |
|        88 | 337          |
|        91 | 334      |

*** Performance
    Of all the agents, the A* Search agent is generally the fastest at computing and executing
    the best path to the goal. However, if the goal is relatively close -- as in a few spaces
    away from the robots starting location-- then the Hill-Climb agent will be more performant.
    This is due to the fact that a Hill Climb Agent would be instantly able to move towards the
    goal location without looking at neighboring locations and creating a command list. 
*** Scalability
Both Uniform and A* Search agents can find the specified goal location in a grid
size of over 50 by 50. However, Reflex and Model Based Reflex agents struggle at finding a corner
on a grid size of 25 by 25. Without obstacles. the Hill-climb agent is able to find its goal location
on the "Robot World" map without getting stuck. 
*** Obstacles
    With more obstacles, A* Search is able to find the specified goal
    location with ease and traverse to the goal. If the map size is greater than
    25 by 25, then the  Uniform Cost Search agent takes a long time finding and
    navigating to the goal. When there is a moderate amount of
    obstacles in the domain, the reflex agents constantly get stuck between obstacles. 
* Discussion
** Other Domain
   The other domain was the traveling salesman issue in AI, exploring the use of A* Search and
   Uniform Cost Search to create the most optimal path around connecting cities. Both of the search
   agents used in the other domain make use of a Euclidean distance based heuristic function, and
   take into account what cities are connected when building a optimal path. 
** Real World Application
   With search algorithms being implemented in the "Robot World," as
   well as the other domain, I learned that each type of path-finding AI has its own
   use case. Specifically A* is the best at finding the optimal path and is useful for
   environments where it wont be interrupted by moving obstacles, or ones where you cant
   easily predict the path of. Such real-world applications
   of A* search can include use cases in things such as TAS (Tool Assisted Speedruns)
   -- where a path to a location is precomputed and executed to find the most time optimal path,
   creating a world record in the process-- or
   video game AI. \\
   A use for a Hill-climb agent might come in the form of situations that don't need the most optimal
   path and there aren't any objects, such as a situation where a robot is catching a ball in a open field.
   A real world application for Reflex agents would be in obstacle avoidance systems, where a sensor doesn't
   have time to build a command-list for A* and needs a instant reaction based on sensors. 
